#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Docker é•œåƒåŠ é€Ÿå™¨æµ‹è¯• Web åº”ç”¨
"""

from flask import Flask, render_template, jsonify, request
from flask_cors import CORS
import urllib.request
import urllib.error
import json
import threading
import os
import shutil
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import redis
import pymysql
from pymysql.cursors import DictCursor

app = Flask(__name__)
CORS(app)

# æ•°æ®åº“å’Œ Redis é…ç½®
MYSQL_CONFIG = {
    'host': os.getenv('MYSQL_HOST', 'localhost'),
    'port': int(os.getenv('MYSQL_PORT', 3306)),
    'user': os.getenv('MYSQL_USER', 'mirror_checker'),
    'password': os.getenv('MYSQL_PASSWORD', 'mirror_checker_pass'),
    'database': os.getenv('MYSQL_DATABASE', 'mirror_checker'),
    'charset': 'utf8mb4',
    'cursorclass': DictCursor
}

REDIS_CONFIG = {
    'host': os.getenv('REDIS_HOST', 'localhost'),
    'port': int(os.getenv('REDIS_PORT', 6379)),
    'db': 0,
    'decode_responses': True
}

# Docker é…ç½®è·¯å¾„
DOCKER_DAEMON_JSON = os.getenv('DOCKER_DAEMON_JSON', '/etc/docker/daemon.json')
DOCKER_DAEMON_JSON_BACKUP = os.getenv('DOCKER_DAEMON_JSON_BACKUP', '/etc/docker/daemon.json.bak')
AUTO_UPDATE_DOCKER_CONFIG = os.getenv('AUTO_UPDATE_DOCKER_CONFIG', 'true').lower() == 'true'

# Redis è¿æ¥æ± 
redis_pool = None
redis_client = None

# åˆå§‹åŒ– Redis
def init_redis():
    global redis_pool, redis_client
    try:
        redis_pool = redis.ConnectionPool(**REDIS_CONFIG)
        redis_client = redis.Redis(connection_pool=redis_pool)
        redis_client.ping()
        print("Redis è¿æ¥æˆåŠŸ")
    except Exception as e:
        print(f"Redis è¿æ¥å¤±è´¥: {e}")
        redis_client = None

# è·å– MySQL è¿æ¥
def get_mysql_connection():
    try:
        return pymysql.connect(**MYSQL_CONFIG)
    except Exception as e:
        print(f"MySQL è¿æ¥å¤±è´¥: {e}")
        return None

# é»˜è®¤é•œåƒç«™åˆ—è¡¨
DEFAULT_MIRRORS = [
    "https://docker.1ms.run",
    "https://docker.1panel.live",
    "https://docker.m.ixdev.cn",
    "https://hub.rat.dev",
    "https://docker.xuanyuan.me",
    "https://dockerproxy.net",
    "https://docker.hlmirror.com",
    "https://hub1.nat.tf",
    "https://hub2.nat.tf",
    "https://hub3.nat.tf",
    "https://hub4.nat.tf",
    "https://docker.m.daocloud.io",
    "https://docker.kejilion.pro",
    "https://hub.1panel.dev",
    "https://dockerproxy.cool",
    "https://proxy.vvvv.ee",
    "https://dockerproxy.com",
    "https://docker.mirrors.ustc.edu.cn",
    "https://docker.nju.edu.cn"
]

# æµ‹è¯•ç»“æœç¼“å­˜
test_results_cache: Dict = {
    "results": [],
    "total": 0,
    "available": 0,
    "unavailable": 0,
    "last_update": None,
    "next_update": None
}

# å®šæ—¶ä»»åŠ¡é”ï¼Œé˜²æ­¢å¹¶å‘æµ‹è¯•
test_lock = threading.Lock()


def test_mirror(mirror: str, timeout: int = 5) -> Tuple[bool, str, int]:
    """
    æµ‹è¯•é•œåƒåŠ é€Ÿå™¨æ˜¯å¦å¯ç”¨
    è¿”å›: (æ˜¯å¦å¯ç”¨, çŠ¶æ€ä¿¡æ¯, HTTPçŠ¶æ€ç )
    """
    test_urls = [
        f"{mirror}/v2/",
        f"{mirror}",
    ]
    
    for test_url in test_urls:
        try:
            req = urllib.request.Request(test_url)
            req.add_header('User-Agent', 'Docker-Mirror-Checker/1.0')
            with urllib.request.urlopen(req, timeout=timeout) as response:
                status_code = response.getcode()
                # 200, 301, 302, 401, 404 éƒ½è¡¨ç¤ºæœåŠ¡å¯ç”¨
                if status_code in [200, 301, 302, 401, 404]:
                    return True, "å¯ç”¨", status_code
                elif status_code == 403:
                    return True, "å¯ç”¨ï¼ˆéœ€è¦è®¤è¯ï¼‰", status_code
        except urllib.error.HTTPError as e:
            # HTTP é”™è¯¯ä½†æœåŠ¡å­˜åœ¨
            if e.code in [401, 403, 404]:
                return True, f"å¯ç”¨ï¼ˆHTTP {e.code}ï¼‰", e.code
            return False, f"HTTP é”™è¯¯: {e.code}", e.code
        except urllib.error.URLError as e:
            continue
        except Exception as e:
            continue
    
    return False, "è¿æ¥å¤±è´¥", 0


def test_mirror_detailed(mirror: str, timeout: int = 5, save_to_db: bool = True) -> Dict:
    """è¯¦ç»†æµ‹è¯•é•œåƒåŠ é€Ÿå™¨"""
    start_time = datetime.now()
    is_available, status_msg, status_code = test_mirror(mirror, timeout)
    end_time = datetime.now()
    response_time = (end_time - start_time).total_seconds() * 1000  # æ¯«ç§’
    
    result = {
        "mirror": mirror,
        "available": is_available,
        "status": status_msg,
        "status_code": status_code,
        "response_time": round(response_time, 2),
        "test_time": end_time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # ä¿å­˜åˆ°æ•°æ®åº“
    if save_to_db:
        save_test_result_to_db(result)
    
    return result


def save_test_result_to_db(result: Dict):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ° MySQL"""
    conn = get_mysql_connection()
    if not conn:
        return
    
    try:
        with conn.cursor() as cursor:
            # æ’å…¥æµ‹è¯•å†å²è®°å½•
            sql = """
                INSERT INTO mirror_test_history 
                (mirror_url, available, status, status_code, response_time, test_time)
                VALUES (%s, %s, %s, %s, %s, %s)
            """
            cursor.execute(sql, (
                result['mirror'],
                result['available'],
                result['status'],
                result['status_code'],
                result['response_time'],
                datetime.strptime(result['test_time'], '%Y-%m-%d %H:%M:%S')
            ))
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            sql_stat = """
                INSERT INTO mirror_statistics 
                (mirror_url, total_tests, success_count, fail_count, avg_response_time, 
                 last_success_time, last_fail_time, current_status)
                VALUES (%s, 1, %s, %s, %s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                    total_tests = total_tests + 1,
                    success_count = success_count + %s,
                    fail_count = fail_count + %s,
                    avg_response_time = (avg_response_time * (total_tests - 1) + %s) / total_tests,
                    last_success_time = IF(%s = 1, %s, last_success_time),
                    last_fail_time = IF(%s = 0, %s, last_fail_time),
                    current_status = %s
            """
            test_time = datetime.strptime(result['test_time'], '%Y-%m-%d %H:%M:%S')
            cursor.execute(sql_stat, (
                result['mirror'],
                1 if result['available'] else 0,
                0 if result['available'] else 1,
                result['response_time'],
                test_time if result['available'] else None,
                test_time if not result['available'] else None,
                result['available'],
                1 if result['available'] else 0,
                0 if result['available'] else 1,
                result['response_time'],
                1 if result['available'] else 0,
                test_time,
                1 if result['available'] else 0,
                test_time,
                result['available']
            ))
        
        conn.commit()
    except Exception as e:
        print(f"ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        conn.rollback()
    finally:
        conn.close()


def test_all_mirrors_background(mirrors: List[str] = None, save_to_db: bool = True) -> Dict:
    """åå°æµ‹è¯•æ‰€æœ‰é•œåƒç«™ï¼ˆç”¨äºå®šæ—¶ä»»åŠ¡ï¼‰"""
    if mirrors is None:
        mirrors = DEFAULT_MIRRORS
    
    results = []
    batch_time = datetime.now()
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæµ‹è¯•
    def test_worker(mirror):
        result = test_mirror_detailed(mirror, save_to_db=save_to_db)
        results.append(result)
    
    threads = []
    for mirror in mirrors:
        thread = threading.Thread(target=test_worker, args=(mirror,))
        thread.start()
        threads.append(thread)
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join(timeout=10)  # æ¯ä¸ªçº¿ç¨‹æœ€å¤šç­‰å¾…10ç§’
    
    # æŒ‰å¯ç”¨æ€§æ’åºï¼šå¯ç”¨çš„åœ¨å‰
    results.sort(key=lambda x: (not x['available'], x['response_time']))
    
    test_result = {
        "results": results,
        "total": len(results),
        "available": sum(1 for r in results if r['available']),
        "unavailable": sum(1 for r in results if not r['available'])
    }
    
    # ä¿å­˜æ‰¹æ¬¡ä¿¡æ¯åˆ°æ•°æ®åº“
    if save_to_db:
        save_batch_to_db(batch_time, test_result)
    
    # ç¼“å­˜åˆ° Redisï¼ˆ1å°æ—¶è¿‡æœŸï¼‰
    cache_to_redis(test_result)
    
    # è‡ªåŠ¨æ›´æ–° Docker é…ç½®
    if AUTO_UPDATE_DOCKER_CONFIG:
        auto_update_docker_config(test_result)
    
    return test_result


def save_batch_to_db(batch_time: datetime, test_result: Dict):
    """ä¿å­˜æ£€æµ‹æ‰¹æ¬¡åˆ°æ•°æ®åº“"""
    conn = get_mysql_connection()
    if not conn:
        return
    
    try:
        with conn.cursor() as cursor:
            sql = """
                INSERT INTO test_batches 
                (batch_time, total_mirrors, available_count, unavailable_count)
                VALUES (%s, %s, %s, %s)
            """
            cursor.execute(sql, (
                batch_time,
                test_result['total'],
                test_result['available'],
                test_result['unavailable']
            ))
        conn.commit()
    except Exception as e:
        print(f"ä¿å­˜æ‰¹æ¬¡ä¿¡æ¯å¤±è´¥: {e}")
        conn.rollback()
    finally:
        conn.close()


def cache_to_redis(data: Dict):
    """ç¼“å­˜æ•°æ®åˆ° Redis"""
    if not redis_client:
        return
    
    try:
        cache_key = "mirror_test_results"
        cache_data = {
            "results": data["results"],
            "total": data["total"],
            "available": data["available"],
            "unavailable": data["unavailable"],
            "last_update": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "next_update": (datetime.now() + timedelta(hours=1)).strftime("%Y-%m-%d %H:%M:%S")
        }
        redis_client.setex(
            cache_key,
            3600,  # 1å°æ—¶è¿‡æœŸ
            json.dumps(cache_data, ensure_ascii=False)
        )
    except Exception as e:
        print(f"Redis ç¼“å­˜å¤±è´¥: {e}")


def get_from_redis() -> Optional[Dict]:
    """ä» Redis è·å–ç¼“å­˜æ•°æ®"""
    if not redis_client:
        return None
    
    try:
        cache_key = "mirror_test_results"
        cached = redis_client.get(cache_key)
        if cached:
            return json.loads(cached)
    except Exception as e:
        print(f"ä» Redis è·å–æ•°æ®å¤±è´¥: {e}")
    
    return None


def auto_update_docker_config(test_result: Dict):
    """è‡ªåŠ¨æ›´æ–° Docker daemon.json é…ç½®"""
    try:
        # ç­›é€‰å¯ç”¨çš„é•œåƒæº
        available = [r for r in test_result.get("results", []) if r.get('available', False)]
        
        if not available:
            print("æ²¡æœ‰å¯ç”¨çš„é•œåƒæºï¼Œè·³è¿‡é…ç½®æ›´æ–°")
            return
        
        # æŒ‰å“åº”æ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€å¿«çš„ 5 ä¸ª
        sorted_available = sorted(available, key=lambda x: x.get('response_time', 9999))
        recommended = sorted_available[:5]
        
        # ç”Ÿæˆé…ç½®
        config = {
            "registry-mirrors": [r['mirror'] for r in recommended]
        }
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        config_dir = os.path.dirname(DOCKER_DAEMON_JSON)
        if not os.path.exists(config_dir):
            print(f"é…ç½®ç›®å½•ä¸å­˜åœ¨: {config_dir}ï¼Œå°è¯•åˆ›å»º...")
            try:
                os.makedirs(config_dir, exist_ok=True)
            except Exception as e:
                print(f"åˆ›å»ºé…ç½®ç›®å½•å¤±è´¥: {e}")
                return
        
        # å¤‡ä»½ç°æœ‰é…ç½®
        if os.path.exists(DOCKER_DAEMON_JSON):
            try:
                shutil.copy2(DOCKER_DAEMON_JSON, DOCKER_DAEMON_JSON_BACKUP)
                print(f"å·²å¤‡ä»½ç°æœ‰é…ç½®åˆ°: {DOCKER_DAEMON_JSON_BACKUP}")
            except Exception as e:
                print(f"å¤‡ä»½é…ç½®å¤±è´¥: {e}")
        
        # è¯»å–ç°æœ‰é…ç½®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        existing_config = {}
        if os.path.exists(DOCKER_DAEMON_JSON):
            try:
                with open(DOCKER_DAEMON_JSON, 'r', encoding='utf-8') as f:
                    existing_config = json.load(f)
            except Exception as e:
                print(f"è¯»å–ç°æœ‰é…ç½®å¤±è´¥: {e}ï¼Œå°†åˆ›å»ºæ–°é…ç½®")
        
        # åˆå¹¶é…ç½®ï¼ˆä¿ç•™å…¶ä»–è®¾ç½®ï¼‰
        existing_config["registry-mirrors"] = config["registry-mirrors"]
        
        # å†™å…¥æ–°é…ç½®
        try:
            with open(DOCKER_DAEMON_JSON, 'w', encoding='utf-8') as f:
                json.dump(existing_config, f, indent=4, ensure_ascii=False)
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] âœ… Docker é…ç½®å·²è‡ªåŠ¨æ›´æ–°: {DOCKER_DAEMON_JSON}")
            print(f"å·²é…ç½® {len(recommended)} ä¸ªé•œåƒæº: {', '.join([r['mirror'] for r in recommended])}")
            
            # å°è¯•é‡å¯ Docker æœåŠ¡ï¼ˆéœ€è¦ç‰¹æ®Šæƒé™ï¼‰
            restart_docker_service()
            
        except PermissionError:
            print(f"âš ï¸  æƒé™ä¸è¶³ï¼Œæ— æ³•å†™å…¥ {DOCKER_DAEMON_JSON}")
            print("è¯·ç¡®ä¿å®¹å™¨æœ‰æƒé™è®¿é—®è¯¥æ–‡ä»¶ï¼Œæˆ–ä½¿ç”¨ volume æŒ‚è½½")
        except Exception as e:
            print(f"å†™å…¥é…ç½®å¤±è´¥: {e}")
            
    except Exception as e:
        print(f"è‡ªåŠ¨æ›´æ–° Docker é…ç½®å¤±è´¥: {e}")


def restart_docker_service():
    """å°è¯•é‡å¯ Docker æœåŠ¡"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ systemctl å‘½ä»¤
        if shutil.which('systemctl'):
            # åœ¨å®¹å™¨å†…æ— æ³•ç›´æ¥é‡å¯å®¿ä¸»æœºçš„ Dockerï¼Œæ‰€ä»¥åªè¾“å‡ºæç¤º
            print("ğŸ’¡ æç¤º: é…ç½®å·²æ›´æ–°ï¼Œè¯·æ‰‹åŠ¨æ‰§è¡Œä»¥ä¸‹å‘½ä»¤é‡å¯ Docker:")
            print("  sudo systemctl daemon-reload")
            print("  sudo systemctl restart docker")
        else:
            # å°è¯•ä½¿ç”¨å…¶ä»–æ–¹å¼
            print("ğŸ’¡ æç¤º: é…ç½®å·²æ›´æ–°ï¼Œè¯·é‡å¯ Docker æœåŠ¡ä»¥ä½¿é…ç½®ç”Ÿæ•ˆ")
    except Exception as e:
        print(f"æ£€æŸ¥ Docker æœåŠ¡çŠ¶æ€å¤±è´¥: {e}")


def scheduled_test():
    """å®šæ—¶æµ‹è¯•ä»»åŠ¡ï¼ˆæ¯1å°æ—¶æ‰§è¡Œä¸€æ¬¡ï¼‰"""
    global test_results_cache
    
    if test_lock.acquire(blocking=False):
        try:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å¼€å§‹å®šæ—¶æ£€æµ‹é•œåƒæºçŠ¶æ€...")
            
            # æ‰§è¡Œæµ‹è¯•ï¼ˆä¿å­˜åˆ°æ•°æ®åº“ï¼‰
            test_result = test_all_mirrors_background(save_to_db=True)
            
            # æ›´æ–°å†…å­˜ç¼“å­˜
            now = datetime.now()
            next_update = datetime.fromtimestamp(now.timestamp() + 3600)  # 1å°æ—¶å
            
            test_results_cache = {
                "results": test_result["results"],
                "total": test_result["total"],
                "available": test_result["available"],
                "unavailable": test_result["unavailable"],
                "last_update": now.strftime("%Y-%m-%d %H:%M:%S"),
                "next_update": next_update.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å®šæ—¶æ£€æµ‹å®Œæˆ: å¯ç”¨ {test_result['available']}/{test_result['total']} ä¸ªé•œåƒæº")
            
        except Exception as e:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] å®šæ—¶æ£€æµ‹å‡ºé”™: {str(e)}")
        finally:
            test_lock.release()
    
    # å®‰æ’ä¸‹ä¸€æ¬¡æµ‹è¯•ï¼ˆ1å°æ—¶åï¼‰
    timer = threading.Timer(3600.0, scheduled_test)
    timer.daemon = True
    timer.start()


def start_scheduled_test():
    """å¯åŠ¨å®šæ—¶æµ‹è¯•ä»»åŠ¡"""
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡æµ‹è¯•
    scheduled_test()


@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template('index.html')


@app.route('/api/mirrors', methods=['GET'])
def get_mirrors():
    """è·å–é•œåƒç«™åˆ—è¡¨"""
    mirrors = request.args.get('mirrors')
    if mirrors:
        try:
            mirror_list = json.loads(mirrors)
            return jsonify({"mirrors": mirror_list})
        except:
            pass
    return jsonify({"mirrors": DEFAULT_MIRRORS})


@app.route('/api/test', methods=['POST'])
def test_single():
    """æµ‹è¯•å•ä¸ªé•œåƒç«™"""
    data = request.get_json()
    mirror = data.get('mirror')
    
    if not mirror:
        return jsonify({"error": "ç¼ºå°‘ mirror å‚æ•°"}), 400
    
    result = test_mirror_detailed(mirror)
    return jsonify(result)


@app.route('/api/test/all', methods=['POST'])
def test_all():
    """æµ‹è¯•æ‰€æœ‰é•œåƒç«™ï¼ˆå®æ—¶æµ‹è¯•ï¼‰"""
    data = request.get_json()
    mirrors = data.get('mirrors', DEFAULT_MIRRORS)
    
    if not isinstance(mirrors, list):
        return jsonify({"error": "mirrors å¿…é¡»æ˜¯åˆ—è¡¨"}), 400
    
    results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œæµ‹è¯•ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰
    def test_worker(mirror):
        result = test_mirror_detailed(mirror)
        results.append(result)
    
    threads = []
    for mirror in mirrors:
        thread = threading.Thread(target=test_worker, args=(mirror,))
        thread.start()
        threads.append(thread)
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join(timeout=10)  # æ¯ä¸ªçº¿ç¨‹æœ€å¤šç­‰å¾…10ç§’
    
    # æŒ‰å¯ç”¨æ€§æ’åºï¼šå¯ç”¨çš„åœ¨å‰
    results.sort(key=lambda x: (not x['available'], x['response_time']))
    
    return jsonify({
        "results": results,
        "total": len(results),
        "available": sum(1 for r in results if r['available']),
        "unavailable": sum(1 for r in results if not r['available'])
    })


@app.route('/api/test/cached', methods=['GET'])
def get_cached_results():
    """è·å–ç¼“å­˜çš„æµ‹è¯•ç»“æœï¼ˆä¼˜å…ˆä» Redisï¼Œå…¶æ¬¡å†…å­˜ç¼“å­˜ï¼‰"""
    # å…ˆå°è¯•ä» Redis è·å–
    redis_data = get_from_redis()
    if redis_data:
        return jsonify(redis_data)
    
    # å¦‚æœ Redis æ²¡æœ‰ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜
    return jsonify(test_results_cache)


@app.route('/api/config/recommended', methods=['GET'])
def get_recommended_config():
    """è·å–æ¨èçš„ Docker é…ç½®ï¼ˆåŸºäºæœ€æ–°çš„æ£€æµ‹ç»“æœï¼Œä¼˜å…ˆä» Redisï¼‰"""
    # å…ˆå°è¯•ä» Redis è·å–
    redis_data = get_from_redis()
    if redis_data:
        results = redis_data.get("results", [])
        last_update = redis_data.get("last_update")
        next_update = redis_data.get("next_update")
    else:
        # ä»å†…å­˜ç¼“å­˜è·å–
        results = test_results_cache.get("results", [])
        last_update = test_results_cache.get("last_update")
        next_update = test_results_cache.get("next_update")
    
    if not results:
        return jsonify({
            "error": "æš‚æ— æ£€æµ‹æ•°æ®",
            "config": None
        })
    
    # ç­›é€‰å¯ç”¨çš„é•œåƒæº
    available = [r for r in results if r.get('available', False)]
    
    if not available:
        return jsonify({
            "error": "æš‚æ— å¯ç”¨çš„é•œåƒæº",
            "config": None
        })
    
    # æŒ‰å“åº”æ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€å¿«çš„ 5 ä¸ª
    sorted_available = sorted(available, key=lambda x: x.get('response_time', 9999))
    recommended = sorted_available[:5]
    
    # ç”Ÿæˆé…ç½®
    config = {
        "registry-mirrors": [r['mirror'] for r in recommended]
    }
    
    return jsonify({
        "config": config,
        "mirrors": [r['mirror'] for r in recommended],
        "count": len(recommended),
        "total_available": len(available),
        "last_update": last_update,
        "next_update": next_update
    })


@app.route('/api/config/update', methods=['POST'])
def update_docker_config_manual():
    """æ‰‹åŠ¨è§¦å‘æ›´æ–° Docker é…ç½®"""
    try:
        # è·å–æœ€æ–°çš„æ£€æµ‹ç»“æœ
        redis_data = get_from_redis()
        if redis_data:
            test_result = {
                "results": redis_data.get("results", []),
                "total": redis_data.get("total", 0),
                "available": redis_data.get("available", 0),
                "unavailable": redis_data.get("unavailable", 0)
            }
        else:
            test_result = {
                "results": test_results_cache.get("results", []),
                "total": test_results_cache.get("total", 0),
                "available": test_results_cache.get("available", 0),
                "unavailable": test_results_cache.get("unavailable", 0)
            }
        
        if not test_result.get("results"):
            return jsonify({
                "error": "æš‚æ— æ£€æµ‹æ•°æ®ï¼Œè¯·å…ˆæ‰§è¡Œæ£€æµ‹",
                "success": False
            }), 400
        
        # æ‰§è¡Œè‡ªåŠ¨æ›´æ–°
        auto_update_docker_config(test_result)
        
        return jsonify({
            "success": True,
            "message": "Docker é…ç½®å·²æ›´æ–°",
            "config_path": DOCKER_DAEMON_JSON
        })
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500


@app.route('/api/history', methods=['GET'])
def get_history():
    """è·å–å†å²æ£€æµ‹è®°å½•"""
    mirror_url = request.args.get('mirror')
    limit = int(request.args.get('limit', 100))
    
    conn = get_mysql_connection()
    if not conn:
        return jsonify({"error": "æ•°æ®åº“è¿æ¥å¤±è´¥"}), 500
    
    try:
        with conn.cursor() as cursor:
            if mirror_url:
                sql = """
                    SELECT * FROM mirror_test_history 
                    WHERE mirror_url = %s 
                    ORDER BY test_time DESC 
                    LIMIT %s
                """
                cursor.execute(sql, (mirror_url, limit))
            else:
                sql = """
                    SELECT * FROM mirror_test_history 
                    ORDER BY test_time DESC 
                    LIMIT %s
                """
                cursor.execute(sql, (limit,))
            
            results = cursor.fetchall()
            
            # è½¬æ¢ datetime ä¸ºå­—ç¬¦ä¸²
            for r in results:
                if r.get('test_time'):
                    r['test_time'] = r['test_time'].strftime('%Y-%m-%d %H:%M:%S')
                if r.get('created_at'):
                    r['created_at'] = r['created_at'].strftime('%Y-%m-%d %H:%M:%S')
            
            return jsonify({"history": results})
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    finally:
        conn.close()


@app.route('/api/statistics', methods=['GET'])
def get_statistics():
    """è·å–é•œåƒæºç»Ÿè®¡ä¿¡æ¯"""
    conn = get_mysql_connection()
    if not conn:
        return jsonify({"error": "æ•°æ®åº“è¿æ¥å¤±è´¥"}), 500
    
    try:
        with conn.cursor() as cursor:
            sql = """
                SELECT * FROM mirror_statistics 
                ORDER BY success_count DESC, avg_response_time ASC
            """
            cursor.execute(sql)
            results = cursor.fetchall()
            
            # è½¬æ¢ datetime ä¸ºå­—ç¬¦ä¸²
            for r in results:
                if r.get('last_success_time'):
                    r['last_success_time'] = r['last_success_time'].strftime('%Y-%m-%d %H:%M:%S')
                if r.get('last_fail_time'):
                    r['last_fail_time'] = r['last_fail_time'].strftime('%Y-%m-%d %H:%M:%S')
                if r.get('updated_at'):
                    r['updated_at'] = r['updated_at'].strftime('%Y-%m-%d %H:%M:%S')
            
            return jsonify({"statistics": results})
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    finally:
        conn.close()


@app.route('/api/test/batch', methods=['POST'])
def test_batch():
    """æ‰¹é‡æµ‹è¯•é•œåƒç«™ï¼ˆå¸¦è¿›åº¦ï¼‰"""
    data = request.get_json()
    mirrors = data.get('mirrors', DEFAULT_MIRRORS)
    
    if not isinstance(mirrors, list):
        return jsonify({"error": "mirrors å¿…é¡»æ˜¯åˆ—è¡¨"}), 400
    
    results = []
    completed = 0
    
    for mirror in mirrors:
        result = test_mirror_detailed(mirror)
        results.append(result)
        completed += 1
        
        # è¿”å›è¿›åº¦ï¼ˆæµå¼å“åº”ï¼‰
        yield f"data: {json.dumps({'progress': completed, 'total': len(mirrors), 'result': result})}\n\n"
    
    # æœ€ç»ˆç»“æœ
    results.sort(key=lambda x: (not x['available'], x['response_time']))
    yield f"data: {json.dumps({'done': True, 'results': results, 'total': len(results), 'available': sum(1 for r in results if r['available']), 'unavailable': sum(1 for r in results if not r['available'])})}\n\n"


@app.route('/api/health', methods=['GET'])
def health():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({"status": "ok"})


if __name__ == '__main__':
    # åˆå§‹åŒ– Redis
    print("åˆå§‹åŒ– Redis è¿æ¥...")
    init_redis()
    
    # å°è¯•ä» Redis åŠ è½½ç¼“å­˜
    cached_data = get_from_redis()
    if cached_data:
        test_results_cache.update(cached_data)
        print("ä» Redis åŠ è½½ç¼“å­˜æ•°æ®æˆåŠŸ")
    
    # å¯åŠ¨å®šæ—¶æµ‹è¯•ä»»åŠ¡
    print("å¯åŠ¨å®šæ—¶æ£€æµ‹ä»»åŠ¡ï¼ˆæ¯1å°æ—¶æ£€æµ‹ä¸€æ¬¡ï¼‰...")
    start_scheduled_test()
    
    # å¯åŠ¨ Flask åº”ç”¨
    app.run(host='0.0.0.0', port=5000, debug=False)

